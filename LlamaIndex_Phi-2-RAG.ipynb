{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phi-2\n",
    "#### RAG with LlamaIndex - Nvidia CUDA + WSL (Windows Subsystem for Linux) + Word documents + Local LLM\n",
    "\n",
    "### Note: Uses FP16 model (not quantized) - possible issue with llamaCPP and the quantized versions. I was sometimes not getting a response (even to the same question)\n",
    "\n",
    "This notebook demonstrates the use of LlamaIndex for Retrieval Augmented Generation using Windows' WSL and an Nvidia's CUDA.\n",
    "\n",
    "Environment:\n",
    "- Windows 11\n",
    "- Anaconda environment\n",
    "- Nvidia RTX 3090\n",
    "- LLMs - Phi-2, Mixtral 8x7B, Mistral 7B, Llama 2 13B Chat, Orca 2, Yi 34B - Quantized versions\n",
    "\n",
    "Your Data:\n",
    "- Add Word documents to the \"Data\" folder for the RAG to use\n",
    "\n",
    "Package versions:\n",
    "- See the \"conda_package_versions.txt\" for the full list of versions in the conda environment (generated using \"conda list\").\n",
    "- See/utilise the \"requirements.txt\" file (note that you need to have installed the CUDA Toolkit using the instructions below, the versions are very important).\n",
    "\n",
    "Local LLMs:\n",
    "- I downloaded the quantized versions of the LLMs from huggingface.co - thanks to TheBloke who provided these quantized GGUF models. You can use higher quantized versions or different LLMs - just be aware that LLMs may have different prompt templates so be sure to use the correct prompt template format (e.g. Llama 2 requires a specific format for best results - see the Llama code for a function that creates the prompt).\n",
    "- See the main README for links to all the models that I downloaded\n",
    "\n",
    "Important libraries to \"pip install\":\n",
    "- llama-cpp-python\n",
    "- transformers\n",
    "- llama-index\n",
    "- docx2txt\n",
    "- sentence-transformers\n",
    "\n",
    "Notes:\n",
    "Getting the Nvidia CUDA libraries installed correctly for use within WSL was challenging, I followed the steps from these two links:\n",
    "\n",
    "1. CUDA Toolkit version 12.3 (latest as of 2023-11-03)\n",
    "- https://docs.nvidia.com/cuda/wsl-user-guide/index.html\n",
    "2. Install instructions within WSL:\n",
    "- https://developer.nvidia.com/cuda-downloads?target_os=Linux&target_arch=x86_64&Distribution=WSL-Ubuntu&target_version=2.0&target_type=deb_local\n",
    "\n",
    "To tell if you are utilising your Nvidia graphics card, in your command prompt, while in the conda environment, type \"nvidia-smi\". You should see your graphics card and you're notebook is running you should see your utilisation increase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Prepare Llama Index for use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Load the Word document(s)\n",
    "\n",
    "Note: A fictitious story about Thundertooth a dinosaur who has travelled to the future. Thanks ChatGPT!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(\"./Data/\").load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Instantiate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 18 key-value pairs and 325 tensors from ./Models/phi-2-ggml-model-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight f16      [  2560, 51200,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:             blk.0.attn_norm.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:           blk.0.attn_norm.weight f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:              blk.0.attn_qkv.bias f32      [  7680,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:            blk.0.attn_qkv.weight f16      [  2560,  7680,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:           blk.0.attn_output.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:         blk.0.attn_output.weight f16      [  2560,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:                blk.0.ffn_up.bias f32      [ 10240,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:              blk.0.ffn_up.weight f16      [  2560, 10240,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.ffn_down.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:            blk.0.ffn_down.weight f16      [ 10240,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:             blk.1.attn_norm.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:           blk.1.attn_norm.weight f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.attn_qkv.bias f32      [  7680,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:            blk.1.attn_qkv.weight f16      [  2560,  7680,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:           blk.1.attn_output.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:         blk.1.attn_output.weight f16      [  2560,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:                blk.1.ffn_up.bias f32      [ 10240,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.ffn_up.weight f16      [  2560, 10240,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:              blk.1.ffn_down.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:            blk.1.ffn_down.weight f16      [ 10240,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:            blk.10.attn_norm.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:          blk.10.attn_norm.weight f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:             blk.10.attn_qkv.bias f32      [  7680,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:           blk.10.attn_qkv.weight f16      [  2560,  7680,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:          blk.10.attn_output.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:        blk.10.attn_output.weight f16      [  2560,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:               blk.10.ffn_up.bias f32      [ 10240,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:             blk.10.ffn_up.weight f16      [  2560, 10240,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:             blk.10.ffn_down.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:           blk.10.ffn_down.weight f16      [ 10240,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:            blk.11.attn_norm.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:          blk.11.attn_norm.weight f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:             blk.11.attn_qkv.bias f32      [  7680,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:           blk.11.attn_qkv.weight f16      [  2560,  7680,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:          blk.11.attn_output.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:        blk.11.attn_output.weight f16      [  2560,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:               blk.11.ffn_up.bias f32      [ 10240,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:             blk.11.ffn_up.weight f16      [  2560, 10240,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:             blk.11.ffn_down.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:           blk.11.ffn_down.weight f16      [ 10240,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:            blk.12.attn_norm.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:          blk.12.attn_norm.weight f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:             blk.12.attn_qkv.bias f32      [  7680,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:           blk.12.attn_qkv.weight f16      [  2560,  7680,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:          blk.12.attn_output.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:        blk.12.attn_output.weight f16      [  2560,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:               blk.12.ffn_up.bias f32      [ 10240,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:             blk.12.ffn_up.weight f16      [  2560, 10240,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:             blk.12.ffn_down.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:           blk.12.ffn_down.weight f16      [ 10240,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:            blk.13.attn_norm.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:          blk.13.attn_norm.weight f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:             blk.13.attn_qkv.bias f32      [  7680,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:           blk.13.attn_qkv.weight f16      [  2560,  7680,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:          blk.13.attn_output.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:        blk.13.attn_output.weight f16      [  2560,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:               blk.13.ffn_up.bias f32      [ 10240,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:             blk.13.ffn_up.weight f16      [  2560, 10240,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:             blk.13.ffn_down.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:           blk.13.ffn_down.weight f16      [ 10240,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:            blk.14.attn_norm.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:          blk.14.attn_norm.weight f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:             blk.14.attn_qkv.bias f32      [  7680,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:           blk.14.attn_qkv.weight f16      [  2560,  7680,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:          blk.14.attn_output.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:        blk.14.attn_output.weight f16      [  2560,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:               blk.14.ffn_up.bias f32      [ 10240,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:             blk.14.ffn_up.weight f16      [  2560, 10240,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:             blk.14.ffn_down.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:           blk.14.ffn_down.weight f16      [ 10240,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:            blk.15.attn_norm.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:          blk.15.attn_norm.weight f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:             blk.15.attn_qkv.bias f32      [  7680,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:           blk.15.attn_qkv.weight f16      [  2560,  7680,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:          blk.15.attn_output.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:        blk.15.attn_output.weight f16      [  2560,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:               blk.15.ffn_up.bias f32      [ 10240,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:             blk.15.ffn_up.weight f16      [  2560, 10240,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:             blk.15.ffn_down.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:           blk.15.ffn_down.weight f16      [ 10240,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:            blk.16.attn_norm.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:          blk.16.attn_norm.weight f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:             blk.16.attn_qkv.bias f32      [  7680,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:           blk.16.attn_qkv.weight f16      [  2560,  7680,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:          blk.16.attn_output.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:        blk.16.attn_output.weight f16      [  2560,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:               blk.16.ffn_up.bias f32      [ 10240,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:             blk.16.ffn_up.weight f16      [  2560, 10240,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:             blk.16.ffn_down.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:           blk.16.ffn_down.weight f16      [ 10240,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:            blk.17.attn_norm.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:          blk.17.attn_norm.weight f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:             blk.17.attn_qkv.bias f32      [  7680,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:           blk.17.attn_qkv.weight f16      [  2560,  7680,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:          blk.17.attn_output.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:        blk.17.attn_output.weight f16      [  2560,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:               blk.17.ffn_up.bias f32      [ 10240,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:             blk.17.ffn_up.weight f16      [  2560, 10240,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:             blk.17.ffn_down.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:           blk.17.ffn_down.weight f16      [ 10240,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:            blk.18.attn_norm.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:          blk.18.attn_norm.weight f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:             blk.18.attn_qkv.bias f32      [  7680,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:           blk.18.attn_qkv.weight f16      [  2560,  7680,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:          blk.18.attn_output.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:        blk.18.attn_output.weight f16      [  2560,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:               blk.18.ffn_up.bias f32      [ 10240,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:             blk.18.ffn_up.weight f16      [  2560, 10240,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:             blk.18.ffn_down.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:           blk.18.ffn_down.weight f16      [ 10240,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:            blk.19.attn_norm.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:          blk.19.attn_norm.weight f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:             blk.19.attn_qkv.bias f32      [  7680,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:           blk.19.attn_qkv.weight f16      [  2560,  7680,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:          blk.19.attn_output.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:        blk.19.attn_output.weight f16      [  2560,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:               blk.19.ffn_up.bias f32      [ 10240,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:             blk.19.ffn_up.weight f16      [  2560, 10240,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:             blk.19.ffn_down.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:           blk.19.ffn_down.weight f16      [ 10240,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.2.attn_norm.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:           blk.2.attn_norm.weight f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:              blk.2.attn_qkv.bias f32      [  7680,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:            blk.2.attn_qkv.weight f16      [  2560,  7680,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:           blk.2.attn_output.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:         blk.2.attn_output.weight f16      [  2560,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:                blk.2.ffn_up.bias f32      [ 10240,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:              blk.2.ffn_up.weight f16      [  2560, 10240,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:              blk.2.ffn_down.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:            blk.2.ffn_down.weight f16      [ 10240,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:            blk.20.attn_norm.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:          blk.20.attn_norm.weight f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:             blk.20.attn_qkv.bias f32      [  7680,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:           blk.20.attn_qkv.weight f16      [  2560,  7680,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:          blk.20.attn_output.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:        blk.20.attn_output.weight f16      [  2560,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:               blk.20.ffn_up.bias f32      [ 10240,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:             blk.20.ffn_up.weight f16      [  2560, 10240,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.20.ffn_down.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:           blk.20.ffn_down.weight f16      [ 10240,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:            blk.21.attn_norm.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:          blk.21.attn_norm.weight f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:             blk.21.attn_qkv.bias f32      [  7680,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:           blk.21.attn_qkv.weight f16      [  2560,  7680,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.21.attn_output.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:        blk.21.attn_output.weight f16      [  2560,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:               blk.21.ffn_up.bias f32      [ 10240,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.21.ffn_up.weight f16      [  2560, 10240,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:             blk.21.ffn_down.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:           blk.21.ffn_down.weight f16      [ 10240,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:            blk.22.attn_norm.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:          blk.22.attn_norm.weight f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.22.attn_qkv.bias f32      [  7680,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:           blk.22.attn_qkv.weight f16      [  2560,  7680,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:          blk.22.attn_output.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:        blk.22.attn_output.weight f16      [  2560,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:               blk.22.ffn_up.bias f32      [ 10240,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:             blk.22.ffn_up.weight f16      [  2560, 10240,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:             blk.22.ffn_down.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:           blk.22.ffn_down.weight f16      [ 10240,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:            blk.23.attn_norm.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:          blk.23.attn_norm.weight f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:             blk.23.attn_qkv.bias f32      [  7680,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:           blk.23.attn_qkv.weight f16      [  2560,  7680,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:          blk.23.attn_output.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:        blk.23.attn_output.weight f16      [  2560,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:               blk.23.ffn_up.bias f32      [ 10240,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:             blk.23.ffn_up.weight f16      [  2560, 10240,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:             blk.23.ffn_down.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:           blk.23.ffn_down.weight f16      [ 10240,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:            blk.24.attn_norm.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:          blk.24.attn_norm.weight f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:             blk.24.attn_qkv.bias f32      [  7680,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:           blk.24.attn_qkv.weight f16      [  2560,  7680,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:          blk.24.attn_output.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:        blk.24.attn_output.weight f16      [  2560,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:               blk.24.ffn_up.bias f32      [ 10240,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:             blk.24.ffn_up.weight f16      [  2560, 10240,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:             blk.24.ffn_down.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:           blk.24.ffn_down.weight f16      [ 10240,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:            blk.25.attn_norm.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:          blk.25.attn_norm.weight f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:             blk.25.attn_qkv.bias f32      [  7680,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:           blk.25.attn_qkv.weight f16      [  2560,  7680,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:          blk.25.attn_output.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:        blk.25.attn_output.weight f16      [  2560,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:               blk.25.ffn_up.bias f32      [ 10240,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:             blk.25.ffn_up.weight f16      [  2560, 10240,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:             blk.25.ffn_down.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:           blk.25.ffn_down.weight f16      [ 10240,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:            blk.26.attn_norm.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:          blk.26.attn_norm.weight f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:             blk.26.attn_qkv.bias f32      [  7680,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:           blk.26.attn_qkv.weight f16      [  2560,  7680,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:          blk.26.attn_output.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:        blk.26.attn_output.weight f16      [  2560,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:               blk.26.ffn_up.bias f32      [ 10240,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:             blk.26.ffn_up.weight f16      [  2560, 10240,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:             blk.26.ffn_down.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:           blk.26.ffn_down.weight f16      [ 10240,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:            blk.27.attn_norm.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:          blk.27.attn_norm.weight f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:             blk.27.attn_qkv.bias f32      [  7680,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:           blk.27.attn_qkv.weight f16      [  2560,  7680,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:          blk.27.attn_output.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:        blk.27.attn_output.weight f16      [  2560,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:               blk.27.ffn_up.bias f32      [ 10240,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:             blk.27.ffn_up.weight f16      [  2560, 10240,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:             blk.27.ffn_down.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:           blk.27.ffn_down.weight f16      [ 10240,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:            blk.28.attn_norm.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:          blk.28.attn_norm.weight f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:             blk.28.attn_qkv.bias f32      [  7680,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:           blk.28.attn_qkv.weight f16      [  2560,  7680,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:          blk.28.attn_output.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:        blk.28.attn_output.weight f16      [  2560,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:               blk.28.ffn_up.bias f32      [ 10240,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:             blk.28.ffn_up.weight f16      [  2560, 10240,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:             blk.28.ffn_down.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:           blk.28.ffn_down.weight f16      [ 10240,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:            blk.29.attn_norm.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:          blk.29.attn_norm.weight f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:             blk.29.attn_qkv.bias f32      [  7680,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:           blk.29.attn_qkv.weight f16      [  2560,  7680,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:          blk.29.attn_output.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:        blk.29.attn_output.weight f16      [  2560,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:               blk.29.ffn_up.bias f32      [ 10240,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:             blk.29.ffn_up.weight f16      [  2560, 10240,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:             blk.29.ffn_down.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:           blk.29.ffn_down.weight f16      [ 10240,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:             blk.3.attn_norm.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:           blk.3.attn_norm.weight f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:              blk.3.attn_qkv.bias f32      [  7680,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:            blk.3.attn_qkv.weight f16      [  2560,  7680,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:           blk.3.attn_output.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:         blk.3.attn_output.weight f16      [  2560,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:                blk.3.ffn_up.bias f32      [ 10240,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:              blk.3.ffn_up.weight f16      [  2560, 10240,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:              blk.3.ffn_down.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:            blk.3.ffn_down.weight f16      [ 10240,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:            blk.30.attn_norm.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:          blk.30.attn_norm.weight f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:             blk.4.attn_norm.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:           blk.4.attn_norm.weight f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:              blk.4.attn_qkv.bias f32      [  7680,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:            blk.4.attn_qkv.weight f16      [  2560,  7680,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:           blk.4.attn_output.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:         blk.4.attn_output.weight f16      [  2560,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:                blk.4.ffn_up.bias f32      [ 10240,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:              blk.4.ffn_up.weight f16      [  2560, 10240,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:              blk.4.ffn_down.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:            blk.4.ffn_down.weight f16      [ 10240,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:             blk.5.attn_norm.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:           blk.5.attn_norm.weight f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:              blk.5.attn_qkv.bias f32      [  7680,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:            blk.5.attn_qkv.weight f16      [  2560,  7680,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:           blk.5.attn_output.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:         blk.5.attn_output.weight f16      [  2560,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:                blk.5.ffn_up.bias f32      [ 10240,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:              blk.5.ffn_up.weight f16      [  2560, 10240,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:              blk.5.ffn_down.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:            blk.5.ffn_down.weight f16      [ 10240,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:             blk.6.attn_norm.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:           blk.6.attn_norm.weight f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:              blk.6.attn_qkv.bias f32      [  7680,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:            blk.6.attn_qkv.weight f16      [  2560,  7680,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:           blk.6.attn_output.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:         blk.6.attn_output.weight f16      [  2560,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:                blk.6.ffn_up.bias f32      [ 10240,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:              blk.6.ffn_up.weight f16      [  2560, 10240,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:              blk.6.ffn_down.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:            blk.6.ffn_down.weight f16      [ 10240,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:             blk.7.attn_norm.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:           blk.7.attn_norm.weight f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:              blk.7.attn_qkv.bias f32      [  7680,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:            blk.7.attn_qkv.weight f16      [  2560,  7680,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:           blk.7.attn_output.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:         blk.7.attn_output.weight f16      [  2560,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:                blk.7.ffn_up.bias f32      [ 10240,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:              blk.7.ffn_up.weight f16      [  2560, 10240,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:              blk.7.ffn_down.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:            blk.7.ffn_down.weight f16      [ 10240,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:             blk.8.attn_norm.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:           blk.8.attn_norm.weight f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:              blk.8.attn_qkv.bias f32      [  7680,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:            blk.8.attn_qkv.weight f16      [  2560,  7680,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:           blk.8.attn_output.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:         blk.8.attn_output.weight f16      [  2560,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:                blk.8.ffn_up.bias f32      [ 10240,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:              blk.8.ffn_up.weight f16      [  2560, 10240,     1,     1 ]\n",
      "llama_model_loader: - tensor  291:              blk.8.ffn_down.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  292:            blk.8.ffn_down.weight f16      [ 10240,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  293:             blk.9.attn_norm.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  294:           blk.9.attn_norm.weight f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  295:              blk.9.attn_qkv.bias f32      [  7680,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  296:            blk.9.attn_qkv.weight f16      [  2560,  7680,     1,     1 ]\n",
      "llama_model_loader: - tensor  297:           blk.9.attn_output.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  298:         blk.9.attn_output.weight f16      [  2560,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  299:                blk.9.ffn_up.bias f32      [ 10240,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  300:              blk.9.ffn_up.weight f16      [  2560, 10240,     1,     1 ]\n",
      "llama_model_loader: - tensor  301:              blk.9.ffn_down.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  302:            blk.9.ffn_down.weight f16      [ 10240,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  303:                      output.bias f32      [ 51200,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  304:                    output.weight f16      [  2560, 51200,     1,     1 ]\n",
      "llama_model_loader: - tensor  305:                 output_norm.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  306:               output_norm.weight f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  307:             blk.30.attn_qkv.bias f32      [  7680,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  308:           blk.30.attn_qkv.weight f16      [  2560,  7680,     1,     1 ]\n",
      "llama_model_loader: - tensor  309:          blk.30.attn_output.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  310:        blk.30.attn_output.weight f16      [  2560,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  311:               blk.30.ffn_up.bias f32      [ 10240,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  312:             blk.30.ffn_up.weight f16      [  2560, 10240,     1,     1 ]\n",
      "llama_model_loader: - tensor  313:             blk.30.ffn_down.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  314:           blk.30.ffn_down.weight f16      [ 10240,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  315:            blk.31.attn_norm.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  316:          blk.31.attn_norm.weight f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  317:             blk.31.attn_qkv.bias f32      [  7680,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  318:           blk.31.attn_qkv.weight f16      [  2560,  7680,     1,     1 ]\n",
      "llama_model_loader: - tensor  319:          blk.31.attn_output.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  320:        blk.31.attn_output.weight f16      [  2560,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  321:               blk.31.ffn_up.bias f32      [ 10240,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  322:             blk.31.ffn_up.weight f16      [  2560, 10240,     1,     1 ]\n",
      "llama_model_loader: - tensor  323:             blk.31.ffn_down.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  324:           blk.31.ffn_down.weight f16      [ 10240,  2560,     1,     1 ]\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = phi2\n",
      "llama_model_loader: - kv   1:                               general.name str              = Phi2\n",
      "llama_model_loader: - kv   2:                        phi2.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                      phi2.embedding_length u32              = 2560\n",
      "llama_model_loader: - kv   4:                   phi2.feed_forward_length u32              = 10240\n",
      "llama_model_loader: - kv   5:                           phi2.block_count u32              = 32\n",
      "llama_model_loader: - kv   6:                  phi2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:               phi2.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   8:          phi2.attention.layer_norm_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv   9:                  phi2.rope.dimension_count u32              = 32\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,51200]   = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  13:                  tokenizer.ggml.token_type arr[i32,51200]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.merges arr[str,50000]   = [\"Ġ t\", \"Ġ a\", \"h e\", \"i n\", \"r e\",...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 50256\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 50256\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 50256\n",
      "llama_model_loader: - type  f32:  195 tensors\n",
      "llama_model_loader: - type  f16:  130 tensors\n",
      "llm_load_vocab: mismatch in special tokens definition ( 910/51200 vs 944/51200 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = phi2\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 51200\n",
      "llm_load_print_meta: n_merges         = 50000\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 2560\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 32\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 1.0e-05\n",
      "llm_load_print_meta: f_norm_rms_eps   = 0.0e+00\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 10240\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 3B\n",
      "llm_load_print_meta: model ftype      = F16\n",
      "llm_load_print_meta: model params     = 2.78 B\n",
      "llm_load_print_meta: model size       = 5.18 GiB (16.01 BPW) \n",
      "llm_load_print_meta: general.name     = Phi2\n",
      "llm_load_print_meta: BOS token        = 50256 '<|endoftext|>'\n",
      "llm_load_print_meta: EOS token        = 50256 '<|endoftext|>'\n",
      "llm_load_print_meta: UNK token        = 50256 '<|endoftext|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_tensors: ggml ctx size =    0.12 MiB\n",
      "llm_load_tensors: mem required  = 5303.78 MiB\n",
      ".............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: KV self size  =  640.00 MiB, K (f16):  320.00 MiB, V (f16):  320.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 774/774\n",
      "llama_new_context_with_model: compute buffer total size = 165.19 MiB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from llama_index.llms import LlamaCPP\n",
    "from llama_index.llms.llama_utils import messages_to_prompt, completion_to_prompt\n",
    "llm = LlamaCPP(\n",
    "    model_url=None, # We'll load locally.\n",
    "    model_path='./Models/phi-2-ggml-model-f16.gguf', # Using FP16 (not quantized) model as the quantized versions aren't always providing a response.\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=256,\n",
    "    context_window=2048, # Phi-2 2K context window - this could be a limitation for RAG as it has to put the content into this context window\n",
    "    generate_kwargs={},\n",
    "    # set to at least 1 to use GPU\n",
    "    model_kwargs={\"n_gpu_layers\": 30}, # This is small model and there's no indication of layers offloaded to the GPU\n",
    "    messages_to_prompt=messages_to_prompt,\n",
    "    completion_to_prompt=completion_to_prompt,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Checkpoint\n",
    "\n",
    "Are you running on GPU? The above output should include near the top something like:\n",
    "> ggml_init_cublas: found 1 CUDA devices:\n",
    "\n",
    "And in the full text near the bottom should be:\n",
    "> llm_load_tensors: using CUDA for GPU acceleration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Embeddings\n",
    "\n",
    "Convert your source document text into embeddings.\n",
    "\n",
    "The embedding model is from huggingface, this one performs well.\n",
    "\n",
    "> https://huggingface.co/thenlper/gte-large\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: thenlper/gte-large\n",
      "Load pretrained SentenceTransformer: thenlper/gte-large\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device: cuda\n",
      "Use pytorch device: cuda\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from llama_index import LangchainEmbedding, ServiceContext\n",
    "\n",
    "embed_model = LangchainEmbedding(\n",
    "  HuggingFaceEmbeddings(model_name=\"thenlper/gte-large\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Prompt Template\n",
    "\n",
    "Prompt template for Phi-2 is below. As there's only a prompt we will combine the system message and prompt into the prompt.\n",
    "\n",
    "Instruct: {prompt}<br>\n",
    "Output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produces a prompt specific to the model\n",
    "def modelspecific_prompt(promptmessage):\n",
    "    return f\"Instruct: {promptmessage}\\nOutput:\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Service Context\n",
    "\n",
    "For chunking the document into tokens using the embedding model and our LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_context = ServiceContext.from_defaults(\n",
    "    chunk_size=256, # Number of tokens in each chunk\n",
    "    llm=llm,\n",
    "    embed_model=embed_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Index documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e1008d8c70144d69e68892576b99041",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d03862c36d3f49cdae08f67894955b58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = VectorStoreIndex.from_documents(documents, service_context=service_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Query Engine\n",
    "\n",
    "Create a query engine, specifying how many citations we want to get back from the searched text (in this case 3).\n",
    "\n",
    "The DB_DOC_ID_KEY is used to get back the filename of the original document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.query_engine import CitationQueryEngine\n",
    "query_engine = CitationQueryEngine.from_args(\n",
    "    index,\n",
    "    similarity_top_k=3,\n",
    "    # here we can control how granular citation sources are, the default is 512\n",
    "    citation_chunk_size=256,\n",
    ")\n",
    "\n",
    "# For citations we get the document info\n",
    "DB_DOC_ID_KEY = \"db_document_id\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. Prompt and Response function\n",
    "\n",
    "Pass in a question, get a response back.\n",
    "\n",
    "IMPORTANT: The prompt is set here, adjust it to match what you want the LLM to act like and do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RunQuestion(questionText):\n",
    "    prompt = \"\" #\"You are a story teller who likes to elaborate. Answer questions in a positive, helpful and interesting way. If the answer is not in the following context return ONLY 'Sorry, I don't know the answer to that'.\\n\\nAnswer the following question: \"\n",
    "    prompt = prompt + questionText\n",
    "\n",
    "    queryQuestion = modelspecific_prompt(prompt)\n",
    "\n",
    "    response = query_engine.query(queryQuestion)\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. Questions to test with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "TestQuestions = [\n",
    "    \"Summarise this story for me\",\n",
    "    \"Who was the main protagonist?\",\n",
    "    \"Did they have any children? If so, what were their names?\",\n",
    "    \"Did anything eventful happen?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12. Run Questions through model (this can take a while) and see citations\n",
    "\n",
    "Runs each test question, saves it to a dictionary for output in the last step.\n",
    "\n",
    "Note: Citations are the source documents used and the text the response is based on. This is important for RAG so you can reference these documents for the user, and to ensure it's utilising the right documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1/1: Summarise this story for me\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0026e95d4c7f4f7db8dfbb3897b6e5c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/3: |Thundertooth Part 3.docx| Source 1:\n",
      "Thundertooth\n",
      "\n",
      "\n",
      "\n",
      "One fateful day, as the citizens of the futuristic city went about their daily lives, a collective gasp echoed through the streets as a massive meteor hurtled towards Earth. Panic spread like wildfire as people looked to the sky in horror, realizing the impending catastrophe. The city's advanced technology detected the threat, and an emergency broadcast echoed through the streets, urging everyone to seek shelter.\n",
      "\n",
      "\n",
      "\n",
      "Thundertooth, ever the protector of his newfound home, wasted no time. With a determined gleam in his eyes, he gathered his family and hurried to the city's command center, where Mayor Grace and the leading scientists were coordinating the evacuation efforts.\n",
      "\n",
      "\n",
      "\n",
      "The mayor, recognizing Thundertooth's intelligence and resourcefulness, approached him. \"Thundertooth, we need a plan to divert or neutralize the meteor. Our technology can only do so much, but with your unique abilities, perhaps we can find a solution.\"\n",
      "\n",
      "\n",
      "\n",
      "Thundertooth nodded, understanding the gravity of the situation. He gathered Lumina, Echo, Sapphire, and Ignis, explaining the urgency and the role each of them would play in the impending crisis.\n",
      "\n",
      "2/3: |Thundertooth Part 3.docx| Source 2:\n",
      "Thundertooth stood at the forefront, using his mighty roar to coordinate and inspire the efforts of the city's inhabitants. The ground trembled as the meteor drew closer, but the Thundertooth family's coordinated efforts began to take effect. Lumina's force field shimmered to life, deflecting the meteor's deadly path. Echo's amplified warnings reached every corner of the city, ensuring that no one was left behind.\n",
      "\n",
      "\n",
      "\n",
      "As Ignis's controlled bursts of flames interacted with the meteor, it began to change course. The combined efforts of the Thundertooth family, guided by their unique talents, diverted the catastrophic collision. The meteor, once destined for destruction, now harmlessly sailed past the Earth, leaving the city and its inhabitants unscathed.\n",
      "\n",
      "\n",
      "\n",
      "The citizens, emerging from their shelters, erupted into cheers of gratitude. Mayor Grace approached Thundertooth, expressing her heartfelt thanks for the family's heroic efforts. The Thundertooth family, tired but triumphant, basked in the relief of having saved their beloved city from imminent disaster.\n",
      "\n",
      "3/3: |Thundertooth Part 3.docx| Source 3:\n",
      "1. **Lumina**: Utilizing her deep understanding of technology, Lumina would enhance the city's energy systems to generate a powerful force field, providing a protective barrier against the meteor's impact.\n",
      "\n",
      "\n",
      "\n",
      "2. **Echo**: With his extraordinary mimicry abilities, Echo would amplify the emergency signals, ensuring that every citizen received timely warnings and instructions for evacuation.\n",
      "\n",
      "\n",
      "\n",
      "3. **Sapphire**: Harnessing her calming and healing powers, Sapphire would assist in calming the panicked masses, ensuring an orderly and efficient evacuation.\n",
      "\n",
      "\n",
      "\n",
      "4. **Ignis**: Drawing upon his fiery talents, Ignis would create controlled bursts of heat, attempting to alter the meteor's trajectory and reduce its destructive force.\n",
      "\n",
      "\n",
      "\n",
      "As the citizens evacuated to designated shelters, the Thundertooth family sprang into action. Lumina worked tirelessly to strengthen the city's energy systems, Echo echoed evacuation orders through the city's speakers, Sapphire offered comfort to those in distress, and Ignis unleashed controlled bursts of flames towards the approaching meteor.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   19498.87 ms\n",
      "llama_print_timings:      sample time =      45.65 ms /   102 runs   (    0.45 ms per token,  2234.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =     442.24 ms /     9 tokens (   49.14 ms per token,    20.35 tokens per second)\n",
      "llama_print_timings:        eval time =   23614.77 ms /   101 runs   (  233.81 ms per token,     4.28 tokens per second)\n",
      "llama_print_timings:       total time =   24390.68 ms\n"
     ]
    }
   ],
   "source": [
    "qa_pairs = []\n",
    "\n",
    "for index, question in enumerate(TestQuestions, start=1):\n",
    "    question = question.strip() # Clean up\n",
    "\n",
    "    print(f\"\\n{index}/{len(TestQuestions)}: {question}\")\n",
    "\n",
    "    response = RunQuestion(question) # Query and get  response\n",
    "\n",
    "    qa_pairs.append((question.strip(), str(response).strip())) # Add to our output array\n",
    "\n",
    "    # Displays the citations\n",
    "    for index, node in enumerate(response.source_nodes, start=1):\n",
    "        print(f\"{index}/{len(response.source_nodes)}: |{node.node.metadata['file_name']}| {node.node.get_text()}\")\n",
    "\n",
    "    # Uncomment the following line if you want to test just the first question\n",
    "    # break "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13. Output responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 Summarise this story for me\n",
      "\n",
      "The Thundertooth family, consisting of Lumina, Echo, Sapphire, and Ignis, used their unique abilities to divert a massive meteor that was heading towards Earth. With Lumina's force field, Echo's amplified warnings, Sapphire's calming presence, and Ignis's controlled bursts of flames, they successfully changed the course of the meteor, saving the city from destruction. The citizens expressed their gratitude, and the Thundertooth family basked in the relief of having protected their beloved home.\n",
      "\n",
      "--------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index, (question, answer) in enumerate(qa_pairs, start=1):\n",
    "    print(f\"{index}/{len(qa_pairs)} {question}\\n\\n{answer}\\n\\n--------\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamaindexgeneric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
