For llama-cpp-python installed with cuda I had to run the following:
$ CUDACXX=/usr/local/cuda-12.3/bin/nvcc
$ export CUDA_HOME=/usr/local/cuda-12.3
$ export PATH=/usr/local/cuda-12.3/bin:$PATH
$ export LD_LIBRARY_PATH=/usr/local/cuda-12.3/lib:$LD_LIBRARY_PATH
$ nvcc --version
<This should show that it's version 12.3 (and not 10.1)>
$ CUDACXX=/usr/local/cuda-12.3/bin/nvcc CMAKE_ARGS="-DLLAMA_CUBLAS=on -DCMAKE_CUDA_ARCHITECTURES=all-major" FORCE_CMAKE=1 pip install llama-cpp-python --no-cache-dir --force-reinstall --upgrade